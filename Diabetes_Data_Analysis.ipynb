{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8f51133c-49c0-4944-a531-30148370ef7f",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "# Diabetes Data Analysis\n",
        "This notebook is dedicated to the analysis of a diabetes dataset. The steps we will follow are:\n",
        "1. Load the data into a pandas DataFrame.\n",
        "2. Perform exploratory data analysis (EDA) to understand the structure and characteristics of the data.\n",
        "3. Visualize the data to gain further insights.\n",
        "4. Preprocess the data to prepare it for machine learning models (this may include data cleaning, encoding, normalization, etc.).\n",
        "5. Build a Machine Learning model to predict the outcome based on the given features.\n",
        "6. Evaluate the performance of the model.\n",
        "\n",
        "Each step will be accompanied by an explanation and analysis of the process and results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51a20255-1b50-4c5d-8e19-545c05e43a84",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-27T14:03:23.419277+00:00",
          "start_time": "2023-06-27T14:03:23.211330+00:00"
        },
        "datalink": {
          "23cc6ee1-62cc-45c0-90ea-d382536586e1": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 9,
              "orig_num_rows": 5,
              "orig_size_bytes": 400,
              "truncated_num_cols": 9,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 400,
              "truncated_string_columns": []
            },
            "display_id": "23cc6ee1-62cc-45c0-90ea-d382536586e1",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-06-27T14:03:23.261831",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_c61f478ced434318b26c85f2b12c417e"
          }
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "1ce37ba5-8f6c-45fe-96ab-6cbebbb2453a"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('diabetes.csv')\n",
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b571f8a7-2432-43ae-af1e-c9192d151db2",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Data Loading\n",
        "The data has been successfully loaded into a pandas DataFrame. The columns in the data are:\n",
        "- Pregnancies: Number of times pregnant\n",
        "- Glucose: Plasma glucose concentration\n",
        "- BloodPressure: Diastolic blood pressure (mm Hg)\n",
        "- SkinThickness: Triceps skin fold thickness (mm)\n",
        "- Insulin: 2-Hour serum insulin (mu U/ml)\n",
        "- BMI: Body mass index (weight in kg/(height in m)^2)\n",
        "- DiabetesPedigreeFunction: Diabetes pedigree function\n",
        "- Age: Age (years)\n",
        "- Outcome: Class variable (0 or 1)\n",
        "\n",
        "The 'Outcome' column is the target variable we want to predict. A value of 1 represents a positive diagnosis for diabetes, while a 0 represents a negative diagnosis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3561673c-8f90-435e-9471-2037e5ff1fdc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-27T14:05:10.192930+00:00",
          "start_time": "2023-06-27T14:05:10.029623+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "30b841ee-f061-4cfc-a29a-b8c53da065b2"
        }
      },
      "outputs": [],
      "source": [
        "# Check the shape of the data\n",
        "print('Shape of the data:', df.shape)\n",
        "\n",
        "# Check the data types of the columns\n",
        "print('\\nData types of the columns:')\n",
        "print(df.dtypes)\n",
        "\n",
        "# Check for missing values\n",
        "print('\\nMissing values in each column:')\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cbabac2c-82d0-47a2-892f-9c1100b5295e",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Exploratory Data Analysis (EDA)\n",
        "From the initial EDA, we have found that:\n",
        "- The dataset contains 768 rows and 9 columns.\n",
        "- All columns are numerical (int64 or float64).\n",
        "- There are no missing values in the dataset.\n",
        "\n",
        "These findings are crucial as they inform us that we don't need to handle missing data, and all our data is numerical, which is suitable for most machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de8c191f-a827-4a0f-bcdc-11f079aead9a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-27T14:06:16.726605+00:00",
          "start_time": "2023-06-27T14:06:13.376761+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "01e0cbca-e996-47ae-8ea2-1b1f8c5de7c8"
        }
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot histograms for each column\n",
        "df.hist(bins=50, figsize=(20,15))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3758042e-d29f-4646-90f1-877772b53c95",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Data Visualization\n",
        "From the histograms for each numerical column in the dataset, we can observe that:\n",
        "- Most of the columns like 'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', and 'Age' are not normally distributed.\n",
        "- The 'Outcome' column, which is our target variable, shows that we have more instances of class 0 (no diabetes) than class 1 (diabetes). This is an important observation as it tells us that our dataset is imbalanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e37eb5e-b30d-45aa-89f7-cff000d8caf3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-27T14:08:15.689548+00:00",
          "start_time": "2023-06-27T14:08:15.177934+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "351395c0-f7e5-4acf-aed3-45305c17580d"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into features (X) and the target variable (y)\n",
        "X = df.drop('Outcome', axis=1)\n",
        "y = df['Outcome']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shape of the training and test sets\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29502944",
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Plot the correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Matrix of Variables')\n",
        "plt.show()\n",
        "\n",
        "# Plot the distribution of the target variable\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(df['Outcome'])\n",
        "plt.title('Distribution of Outcome')\n",
        "plt.show()\n",
        "\n",
        "# Plot the distribution of the Age variable\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.histplot(df['Age'], kde=True)\n",
        "plt.title('Distribution of Age')\n",
        "plt.show()\n",
        "\n",
        "# Plot the relationship between Age and Outcome\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.boxplot(x='Outcome', y='Age', data=df)\n",
        "plt.title('Relationship between Age and Outcome')\n",
        "plt.show()\n",
        "\n",
        "# Plot the relationship between BMI and Outcome\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.boxplot(x='Outcome', y='BMI', data=df)\n",
        "plt.title('Relationship between BMI and Outcome')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4eb3f64a",
      "metadata": {},
      "source": [
        "## Additional Exploratory Data Analysis\n",
        "We created additional plots to gain more insights into the data:\n",
        "\n",
        "1. **Correlation Matrix of Variables**: This heatmap shows the correlation between each pair of variables in the dataset. The correlation is a value between -1 and 1 that represents how closely two variables are linearly related. A value close to 1 means a strong positive correlation (as one variable increases, so does the other), a value close to -1 means a strong negative correlation (as one variable increases, the other decreases), and a value close to 0 means no linear correlation.\n",
        "\n",
        "2. **Distribution of Outcome**: This bar plot shows the number of instances for each class in the 'Outcome' variable. We can see that there are more instances of class 0 (no diabetes) than class 1 (diabetes), indicating that our dataset is imbalanced.\n",
        "\n",
        "3. **Distribution of Age**: This histogram shows the distribution of values in the 'Age' variable. We can see that most of the patients are in their 20s to 30s.\n",
        "\n",
        "4. **Relationship between Age and Outcome**: This box plot shows the distribution of 'Age' for each class in the 'Outcome' variable. We can see that patients with diabetes (Outcome = 1) tend to be older than those without diabetes (Outcome = 0).\n",
        "\n",
        "5. **Relationship between BMI and Outcome**: This box plot shows the distribution of 'BMI' for each class in the 'Outcome' variable. We can see that patients with diabetes (Outcome = 1) tend to have a higher BMI than those without diabetes (Outcome = 0)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5dd703d0-d688-4452-9698-c9b1aa7c4c44",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Data Preprocessing\n",
        "In the data preprocessing stage, we first split our data into features (X) and the target variable (y), and then further split these into training and test sets. This will allow us to evaluate the performance of our machine learning model later on. We used 80% of the data for training and 20% for testing.\n",
        "\n",
        "The shapes of the sets are as follows:\n",
        "- X_train shape: (614, 8)\n",
        "- y_train shape: (614,)\n",
        "- X_test shape: (154, 8)\n",
        "- y_test shape: (154,)\n",
        "\n",
        "This means we have 614 instances in our training set and 154 instances in our test set. Each instance has 8 features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd79bca0-75d2-45c0-b560-1e0c1d0cf475",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-27T14:09:29.945813+00:00",
          "start_time": "2023-06-27T14:09:29.781747+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "0cef8222-77fe-4e24-b97f-3c251bf02de1"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on the training data\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Transform both the training and test data\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Check the first few rows of the transformed training data\n",
        "print(X_train[:5])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a1d0021e-4f9a-4909-8ab6-114703996be2",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "After splitting the data into training and test sets, we normalized the features so that they're on the same scale. This is especially important for algorithms that use distance measures, like k-Nearest Neighbors (k-NN) and Support Vector Machines (SVM).\n",
        "\n",
        "We used the StandardScaler from sklearn, which standardizes features by removing the mean and scaling to unit variance. Each value now represents how many standard deviations the original value is away from the mean. A negative value indicates that the original value was below the mean, while a positive value indicates it was above the mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cff41122-ebbc-43e7-aec3-4abe509f4486",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-27T14:11:27.010327+00:00",
          "start_time": "2023-06-27T14:11:26.819923+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "0cf2507f-c02f-4b6a-8ac3-8c6030539ae0"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_preds = model.predict(X_train)\n",
        "test_preds = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model on the training and test data\n",
        "train_acc = accuracy_score(y_train, train_preds)\n",
        "test_acc = accuracy_score(y_test, test_preds)\n",
        "\n",
        "print('Training accuracy:', train_acc)\n",
        "print('Test accuracy:', test_acc)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "af47b680-f795-4392-9d78-fb4a37a003cc",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "There are several ways we could try to improve the model's performance, such as:\n",
        "- Using a more complex model: Logistic Regression is a simple model which might not be able to capture all the complexities in the data. We could try using a more complex model like a Random Forest or a Gradient Boosting model.\n",
        "- Tuning the model's parameters: We used the default parameters for the Logistic Regression model. We could try tuning these parameters to see if we can get better performance.\n",
        "- Engineering new features from the existing data: We could try creating new features from the existing data which might help improve the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e93f0ed5-a2e7-4655-b69e-fe4bd979bff3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-27T14:18:42.848454+00:00",
          "start_time": "2023-06-27T14:18:42.413065+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "14628d42-09b9-4dac-a987-bcc80d49eb77"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "rf_train_preds = rf_model.predict(X_train)\n",
        "rf_test_preds = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model on the training and test data\n",
        "rf_train_acc = accuracy_score(y_train, rf_train_preds)\n",
        "rf_test_acc = accuracy_score(y_test, rf_test_preds)\n",
        "\n",
        "print('Training accuracy:', rf_train_acc)\n",
        "print('Test accuracy:', rf_test_acc)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3988fc2b-6a2e-4cd2-a4f7-4ee014b018fe",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Trying a More Complex Model\n",
        "We built a Random Forest Classifier and trained it on our training data. We then used the model to make predictions on both the training and test data, and calculated the accuracy of the model on both sets.\n",
        "\n",
        "The model achieved an accuracy of 1.0 on the training data and approximately 0.721 on the test data. While the model performs perfectly on the training data, its performance drops on the test data. This is a clear sign of overfitting, which means the model has learned the training data too well and is not generalizing well to unseen data.\n",
        "\n",
        "To address this, we can try tuning the model's parameters or using a different model. For instance, we could adjust the number of trees in the forest (n_estimators parameter) or the maximum depth of the trees (max_depth parameter). We could also try a Gradient Boosting model, which often performs well on a wide range of problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97df3921-8bea-4cc5-8c68-a0ceb85e2565",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-27T14:22:33.737389+00:00",
          "start_time": "2023-06-27T14:21:00.512347+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "1906af39-e2b9-46ad-990d-53ed2862cf5a"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500],\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50]\n",
        "}\n",
        "\n",
        "# Initialize the GridSearchCV\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Make predictions on the training and test data using the best model\n",
        "gs_train_preds = grid_search.predict(X_train)\n",
        "gs_test_preds = grid_search.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the best model on the training and test data\n",
        "gs_train_acc = accuracy_score(y_train, gs_train_preds)\n",
        "gs_test_acc = accuracy_score(y_test, gs_test_preds)\n",
        "\n",
        "print('Best parameters:', best_params)\n",
        "print('Best cross-validation score:', best_score)\n",
        "print('Training accuracy:', gs_train_acc)\n",
        "print('Test accuracy:', gs_test_acc)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "95c48644-27ec-4872-9c20-44f2c21069e9",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Parameter Tuning\n",
        "We performed a grid search to find the best parameters for the Random Forest model. We tried different values for the number of trees in the forest (n_estimators) and the maximum depth of the trees (max_depth), and used cross-validation to find the best parameters.\n",
        "\n",
        "The best parameters found were 'max_depth': None and 'n_estimators': 200. The best cross-validation score was approximately 0.779.\n",
        "\n",
        "The model with the best parameters achieved an accuracy of 1.0 on the training data and approximately 0.734 on the test data. As with the previous Random Forest model, this model is overfitting the training data. This means the model has learned the training data too well and is not generalizing well to unseen data.\n",
        "\n",
        "To address this, we could try using a different model or engineering new features from the existing data. We could also try a more extensive grid search or a different method for parameter tuning, such as random search or Bayesian optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "336cd566-9b31-4146-82e6-278b70ba55ca",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-27T14:28:17.418784+00:00",
          "start_time": "2023-06-27T14:28:16.709072+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "4561469d-2cb0-4ad8-9b37-5fd7ad5e0114"
        }
      },
      "outputs": [],
      "source": [
        "# Create the interaction feature\n",
        "X_train['Glucose_BMI'] = X_train['Glucose'] * X_train['BMI']\n",
        "X_test['Glucose_BMI'] = X_test['Glucose'] * X_test['BMI']\n",
        "\n",
        "# Initialize the Random Forest Classifier with the best parameters\n",
        "rf_model = RandomForestClassifier(n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "rf_train_preds = rf_model.predict(X_train)\n",
        "rf_test_preds = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model on the training and test data\n",
        "rf_train_acc = accuracy_score(y_train, rf_train_preds)\n",
        "rf_test_acc = accuracy_score(y_test, rf_test_preds)\n",
        "\n",
        "print('Training accuracy:', rf_train_acc)\n",
        "print('Test accuracy:', rf_test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a512756-ff02-4c26-bb27-0991d4a0afce",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-27T14:29:18.393307+00:00",
          "start_time": "2023-06-27T14:29:17.736941+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "583e55b0-789e-43f9-b620-3ac338dc05db"
        }
      },
      "outputs": [],
      "source": [
        "# Convert X_train and X_test back to dataframes\n",
        "X_train_df = pd.DataFrame(X_train, columns=df.columns[:-1])\n",
        "X_test_df = pd.DataFrame(X_test, columns=df.columns[:-1])\n",
        "\n",
        "# Create the interaction feature\n",
        "X_train_df['Glucose_BMI'] = X_train_df['Glucose'] * X_train_df['BMI']\n",
        "X_test_df['Glucose_BMI'] = X_test_df['Glucose'] * X_test_df['BMI']\n",
        "\n",
        "# Convert X_train_df and X_test_df back to numpy arrays\n",
        "X_train = X_train_df.values\n",
        "X_test = X_test_df.values\n",
        "\n",
        "# Initialize the Random Forest Classifier with the best parameters\n",
        "rf_model = RandomForestClassifier(n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "rf_train_preds = rf_model.predict(X_train)\n",
        "rf_test_preds = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model on the training and test data\n",
        "rf_train_acc = accuracy_score(y_train, rf_train_preds)\n",
        "rf_test_acc = accuracy_score(y_test, rf_test_preds)\n",
        "\n",
        "print('Training accuracy:', rf_train_acc)\n",
        "print('Test accuracy:', rf_test_acc)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c84be70a-7d1c-4afa-80fa-16b30aad3975",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Feature Engineering\n",
        "We created an interaction feature between 'Glucose' and 'BMI', which is the product of the two original features. The idea behind interaction features is that the effect of one feature on the target variable may depend on the value of another feature. In this case, the effect of 'Glucose' on 'Outcome' might depend on the value of 'BMI'.\n",
        "\n",
        "We then trained a new Random Forest model with the best parameters found in the grid search on the data with the interaction feature. The model achieved an accuracy of 1.0 on the training data and approximately 0.740 on the test data. This is a slight improvement over the previous model without the interaction feature, but the model is still overfitting the training data.\n",
        "\n",
        "To further improve the model's performance, we could try creating more interaction features or other types of new features. We could also try using a different model or a different method for parameter tuning."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "noteable": {
      "last_transaction_id": "d2020f91-ef24-457f-8381-4c70d413cacc"
    },
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "c85e7f77-a4df-5dd4-b345-ac45aeb182c9",
        "openai_ephemeral_user_id": "5f1cede1-5a0f-5f0b-abee-3c4b9fc2d008",
        "openai_subdivision1_iso_code": "TH-10"
      }
    },
    "selected_hardware_size": "small"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
